{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c13ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 21:31:15.675823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 21:31:16.243879: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-20 21:31:16.243921: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-20 21:31:17.964943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 21:31:17.965070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-20 21:31:17.965083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3b0feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 21:31:20.221962: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-20 21:31:20.222349: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-20 21:31:20.222385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Nakama): /proc/driver/nvidia/version does not exist\n",
      "2022-12-20 21:31:20.223321: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "images = load_sample_images()['images']\n",
    "images = tf.keras.layers.CenterCrop(height=70, width=120)(images)\n",
    "images = tf.keras.layers.Rescaling(scale=1/255)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e43ab9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fac5e",
   "metadata": {},
   "source": [
    "the dimensions show that:\n",
    "    - 2 represents the number of images.\n",
    "    - 70 x 120 is the size we specified.\n",
    "    - 3 represents the color channel (RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49d53b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = tf.keras.layers.Conv2D(filters=32, kernel_size=7, padding='same')\n",
    "fmaps = conv_layer(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9675ddb",
   "metadata": {},
   "source": [
    "We are using Conv2D as 2D refers to the number of spatial dimensions (height and width). As Conv2D does not add padding by default, we will lose some pixels. Hence we set the padding='same' parameter to add zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41475cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 70, 120, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmaps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c939ac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 3, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels, biases = conv_layer.get_weights()\n",
    "kernels.shape #(kernel_height, kernel_width, input_channels, output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2958ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape #(output_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a17e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no of output channels = no of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40225565",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = tf.keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8141e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth wise max pooling.\n",
    "class DepthPool(tf.keras.layers.Layer):\n",
    "    def __init__(self, pool_size=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pool_size = pool_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        groups = shape[-1] #No. of channel groups\n",
    "        new_shape = tf.concat([shape[:-1], [groups, self.pool_size]], axis = 0)\n",
    "        return tf.reduce_max(tf.reshape(inputs, new_shape), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51032657",
   "metadata": {},
   "source": [
    "## Building ResNet-34 using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33279e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal', use_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19d1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            tf.keras.layers.BatchNormalization()\n",
    "        ]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "            ]\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10c107b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    DefaultConv2D(64, kernel_size=7, strides=2, input_shape=[224,224,3]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fabb18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_filters = 64\n",
    "for filters in [64] * 3 +[128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides =1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a195fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93a0e3",
   "metadata": {},
   "source": [
    "## Using pretrained models from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dde4853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102967424/102967424 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.applications.ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77c23d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_sample_images()['images']\n",
    "images_resized = tf.keras.layers.Resizing(height=224, width=224, crop_to_aspect_ratio=True)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ba7960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.applications.resnet50.preprocess_input(images_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de3ef33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "Y_proba = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97768d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da75bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "35363/35363 [==============================] - 0s 1us/step\n",
      "Image #0\n",
      " n03877845 - palace       54.69%\n",
      " n03781244 - monastery    24.71%\n",
      " n02825657 - bell_cote    18.55%\n",
      "Image #1\n",
      " n04522168 - vase         32.67%\n",
      " n11939491 - daisy        17.82%\n",
      " n03530642 - honeycomb    12.04%\n"
     ]
    }
   ],
   "source": [
    "top_K = tf.keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(f\"Image #{image_index}\")\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(f\" {class_id} - {name:12s} {y_proba:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78a464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
